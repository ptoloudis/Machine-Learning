{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Implement manually the random forest tree method and testing on iris data set \n",
    "\n",
    "2. Compare your implementation to sklearn\n",
    "\n",
    "3. Apply the CART and random forest for Predicting and Forecasting Influenza based on the dataset `h7n9.csv`\n",
    "\n",
    "4. Compare your results with the r-study which is included in the lab 4 folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement manually the random forest tree method and testing on iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of trees in the forest\n",
    "n_trees = 10\n",
    "\n",
    "# Define the maximum depth of each tree\n",
    "max_depth = 5\n",
    "\n",
    "# Define the minimum number of samples required to split an internal node\n",
    "min_samples_split = 2\n",
    "\n",
    "# Define the random state for the random number generator\n",
    "random_state = 42\n",
    "\n",
    "# Create a list to store the decision trees\n",
    "trees = []\n",
    "\n",
    "# Train the decision trees\n",
    "for i in range(n_trees):\n",
    "    # Create a random subset of the training data\n",
    "    idx = np.random.choice(X_train.shape[0], size=X_train.shape[0], replace=True)\n",
    "    X_subset = X_train[idx]\n",
    "    y_subset = y_train[idx]\n",
    "    \n",
    "    # Create a decision tree classifier\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, random_state=random_state)\n",
    "    \n",
    "    # Train the decision tree classifier on the random subset of the training data\n",
    "    tree.fit(X_subset, y_subset)\n",
    "    \n",
    "    # Add the decision tree classifier to the list of trees\n",
    "    trees.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptolo\\AppData\\Local\\Temp\\ipykernel_5836\\970344870.py:7: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  y_pred = mode(predictions, axis=1)[0].ravel()\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data using each decision tree in the forest\n",
    "predictions = np.zeros((X_test.shape[0], n_trees))\n",
    "for i, tree in enumerate(trees):\n",
    "    predictions[:, i] = tree.predict(X_test)\n",
    "\n",
    "# Compute the mode of the predictions for each testing sample\n",
    "y_pred = mode(predictions, axis=1)[0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the random forest classifier\n",
    "my_accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n",
    "print(\"Accuracy:\", my_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compare your implementation to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier with 10 trees\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the random forest classifier on the training data\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the random forest classifier\n",
    "accuracy = rfc.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Apply the CART and random forest for Predicting and Forecasting Influenza based on the dataset `h7n9.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('h7n9.csv')\n",
    "\n",
    "# Impute missing values in the target variable with the most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['outcome'] = imputer.fit_transform(df[['outcome']])\n",
    "\n",
    "# Select the features and target variable\n",
    "X = df[['age']]\n",
    "y = df['outcome']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier and fit it to the training data\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Create a random forest classifier and fit it to the training data\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the decision tree classifier\n",
    "dtc_preds = dtc.predict(X_test)\n",
    "\n",
    "# Make predictions using the random forest classifier\n",
    "rfc_preds = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.6666666666666666\n",
      "Random Forest Accuracy: 0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "dtc_accuracy = dtc.score(X_test, y_test)\n",
    "rfc_accuracy = rfc.score(X_test, y_test)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", dtc_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rfc_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
