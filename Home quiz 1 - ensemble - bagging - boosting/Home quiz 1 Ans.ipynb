{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study the 5 presentations about three models and their performance. I will ask each group to make a presentation of each part. You can enhance the presentation using web resources or the material that I will upload today in piaza."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - CLASSIFICATION AND REGRESSION TREES (CART)\n",
    "Set of supervised learning models used for problems involving classification and regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree\n",
    "1. Sequence of if-else questions about individual features: A Classification Tree consists of a sequence of if-else questions about individual features. These questions are used to split the data into different branches based on the feature values.\n",
    "\n",
    "2. Objective: Infer class labels: The main objective of a Classification Tree is to infer or predict class labels for the target variable. Each leaf node in the tree represents a specific class label.\n",
    "\n",
    "3. Able to capture non-linear relationships between features and labels: Classification Trees are capable of capturing non-linear relationships between features and labels. They can handle complex decision boundaries that are not limited to linear relationships.\n",
    "\n",
    "4. No feature scaling required: Classification Trees do not require feature scaling techniques such as standardization or normalization. The splitting of data is based on the relative ordering of feature values, rather than their specific magnitudes.\n",
    "\n",
    "5. Splitting criteria: Classification Trees use various criteria to determine the best feature and split-point for branching. The most commonly used criteria are Gini impurity and entropy, which measure the impurity or uncertainty of a node. The goal is to find splits that maximize the homogeneity or purity of the resulting child nodes.\n",
    "\n",
    "6. Recursive partitioning: Classification Trees are grown through a process called recursive partitioning. Starting from the root node, the data is split recursively based on the selected splitting criteria until certain stopping conditions are met, such as reaching a minimum number of samples or a maximum depth.\n",
    "\n",
    "7. Pruning: To avoid overfitting, which occurs when the tree becomes too complex and specialized to the training data, pruning techniques can be applied. Pruning involves removing or collapsing nodes to simplify the tree while maintaining its predictive performance.\n",
    "\n",
    "8. Interpretability: One of the advantages of Classification Trees is their interpretability. The sequence of if-else questions in the tree structure provides a clear and intuitive representation of the decision-making process. It allows users to understand the logic behind the classification process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision regions\n",
    "1. Definition: Decision regions refer to the regions in the feature space where all instances are assigned to a specific class label. Each region corresponds to a different class.\n",
    "\n",
    "2. Boundary between Decision Regions: The boundary that separates two decision regions is known as the decision boundary or decision surface. It determines the point at which the classification decision changes from one class to another.\n",
    "\n",
    "3. Classifying New Instances: When a new instance or data point is presented to a trained classification model, it is assigned to the decision region that corresponds to the predicted class label. The decision region is determined based on the feature values of the instance.\n",
    "\n",
    "4. Binary Classification Example: In the case of binary classification, where there are two classes (e.g., Pass or Fail), there will be two decision regions. Each region represents the set of feature values for which the instance is assigned to a specific class label. For instance, one decision region could represent the Pass class, and the other decision region could represent the Fail class.\n",
    "\n",
    "5. Shape and Complexity: The shape and complexity of decision regions depend on the underlying classification algorithm and the relationships between the features and the class labels. Decision regions can take various forms, including simple shapes like rectangles, circles, or more complex and irregular shapes.\n",
    "\n",
    "6. Overlapping Decision Regions: In some cases, decision regions may overlap, meaning that certain regions in the feature space can be classified as belonging to multiple classes. This occurs when there is ambiguity or uncertainty in the classification decision based on the available features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundary\n",
    "1. Definition: The decision boundary, also known as the separating boundary, is the boundary or surface that separates different decision regions in a classification problem. It marks the transition from one class to another in the feature space.\n",
    "\n",
    "2. Separating Different Classes: The decision boundary determines the point at which the classification decision changes from one class to another. It is a line, curve, or higher-dimensional surface that divides the feature space into distinct regions corresponding to different class labels.\n",
    "\n",
    "3. Binary Classification Example: In the case of binary classification, where there are two classes, the decision boundary is a line (in two dimensions) or a hyperplane (in higher dimensions) that separates the instances belonging to different classes. For example, in the Pass or Fail classification example, the decision boundary would be located at 33%, which is the passing mark.\n",
    "\n",
    "4. Multiclass Classification Example: In multiclass classification, where there are more than two classes, the decision boundary can be more complex. It can be a combination of lines, curves, or surfaces that separate the different classes in the feature space. The number and complexity of decision boundaries depend on the specific problem and the relationships between the classes.\n",
    "\n",
    "5.  Linear vs. Non-linear Decision Boundaries: In some cases, the decision boundary can be a linear function (e.g., a straight line or a hyperplane), as in linear classifiers like logistic regression or linear support vector machines. However, in many real-world scenarios, the relationships between features and class labels are non-linear, requiring non-linear decision boundaries. Decision trees, for example, are capable of capturing non-linear decision boundaries by recursively splitting the feature space.\n",
    "\n",
    "6. Visualization: Decision boundaries are often visualized using contour plots, scatter plots, or surface plots, depending on the dimensionality of the feature space. These visualizations help understand how the classifier separates the different classes and provide insights into the decision-making process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression vs classification tree\n",
    "### Logistic Regression:\n",
    "\n",
    "* Decision Boundary: Logistic regression models produce a single decision boundary, which is a linear function separating the feature space into two decision regions.\n",
    "* Linearity: The decision boundary of logistic regression is a straight line in two dimensions or a hyperplane in higher dimensions.\n",
    "* Linear Relationships: Logistic regression assumes linear relationships between the features and the log-odds of the target variable. It models the probability of a binary outcome using a logistic function.\n",
    "* Flexibility: Logistic regression is limited to linear decision boundaries, which can be restrictive when the relationships between features and the target variable are non-linear.\n",
    "\n",
    "### Classification Trees:\n",
    "\n",
    "* Decision Boundary: Classification trees divide the feature space into rectangular regions (or boxes) based on a sequence of if-else questions about individual features. Each region corresponds to a specific class label.\n",
    "* Non-linearity: Classification trees are able to capture non-linear relationships between features and labels, as the decision boundaries can have complex shapes that are not limited to straight lines or hyperplanes.\n",
    "* Flexibility: The rectangular regions formed by classification trees allow for more flexible decision boundaries, which can better capture intricate patterns in the data.\n",
    "* Feature Interactions: Classification trees can also capture interactions between features, as different combinations of feature values can lead to different decision regions.\n",
    "* Interpretability: The sequence of if-else questions in a classification tree provides a transparent and interpretable representation of the decision-making process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building block of Decision Tree\n",
    "1. Root Node: The root node is the topmost node of the decision tree. It has no parent node. The root node represents the entire dataset or a subset of it and poses a question that splits the data into two or more child nodes.\n",
    "\n",
    "1. Internal Nodes: Internal nodes are the intermediate nodes of the decision tree. Each internal node has one parent node and represents a question or a condition based on a specific feature. The internal node's question or condition determines the branching path and leads to two or more child nodes.\n",
    "\n",
    "1. Leaf Nodes (also known as Terminal Nodes): Leaf nodes are the final nodes of the decision tree. They have a parent node but no children nodes. Leaf nodes represent the prediction or class label assigned to the instances that reach that specific node.\n",
    "\n",
    "1. Branches: Branches are the paths connecting nodes in the decision tree. They represent the possible outcomes of the questions or conditions posed by the internal nodes. Each branch leads to a different child node based on the feature value that satisfies the question or condition.\n",
    "\n",
    "1. Splitting Criteria: The splitting criteria determine how the decision tree algorithm selects the best question or condition to split the data at each internal node. Common splitting criteria include information gain, Gini impurity, or entropy, which aim to maximize the homogeneity or purity of the resulting subsets.\n",
    "\n",
    "1. Recursive Nature: The decision tree is grown recursively, starting from the root node and branching out until reaching the leaf nodes. At each internal node, the data is split based on the chosen splitting criteria, and the process continues until a stopping condition is met, such as reaching a maximum depth or a minimum number of instances in the nodes.\n",
    "\n",
    "1. Prediction: When a new instance is presented to a trained decision tree model, it traverses the tree from the root node down to a leaf node based on the feature values of the instance. The prediction or class label associated with the reached leaf node is then assigned to the instance.\n",
    "\n",
    "The building blocks of a decision tree allow for a structured and hierarchical representation of the decision-making process. By asking sequential questions at each internal node, the decision tree progressively splits the data, creating decision regions and assigning predictions to the leaf nodes. This tree-like structure makes decision trees interpretable and provides insights into the important features and their impact on the prediction process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classication-Tree Learning (Working) and Information Gain\n",
    "In classification tree learning, the process of growing nodes is performed recursively, starting from the root node and proceeding down the tree. Each node represents a subset of the data and poses a question or condition based on a specific feature to split the data into two or more child nodes.\n",
    "\n",
    "To determine the best feature and split point at each node, the concept of information gain (IG) is utilized. Information gain measures the amount of information gained about the target variable by observing another random variable or feature. It quantifies the reduction in entropy or impurity achieved by splitting the data based on a particular feature and split point.\n",
    "\n",
    "Entropy is a measure of the impurity or disorder in a set of instances, while Gini impurity is another measure of impurity based on the probability of misclassifying a randomly chosen instance. Information gain aims to maximize the reduction in entropy or Gini impurity, indicating the most informative split.\n",
    "\n",
    "Kullback–Leibler divergence (KL divergence) is a mathematical concept used to measure the difference between two probability distributions. Information gain is sometimes referred to as KL divergence, as it calculates the difference in entropy before and after the split.\n",
    "\n",
    "Mutual information is another related concept that measures the amount of information shared between two random variables. It is the conditional expected value of the KL divergence between the joint distribution and the product of the individual distributions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria to measure the impurity of a node I(node):\n",
    "* Variance (Regression): In regression problems, the impurity of a node is measured using variance. Variance reduction at a node is defined as the total reduction in the variance of the target variable (e.g., the dependent variable) achieved by splitting the data at that node. The goal is to minimize the variance within each node and create splits that result in more homogeneous subsets with respect to the target variable.\n",
    "\n",
    "* Gini Impurity (Classification): In classification problems, Gini impurity is used to measure the impurity of a node. Gini impurity represents the probability of incorrectly classifying a randomly chosen element from the set if it were randomly labeled according to the distribution of labels in the subset. It quantifies the degree of impurity or disorder within the node. The objective is to minimize the Gini impurity by creating splits that lead to more pure and homogeneous subsets with respect to class labels.\n",
    "\n",
    "* Entropy (Classification): Entropy is another measure of impurity used in classification problems. It quantifies the average rate at which information is produced by a stochastic source of data. In the context of decision trees, entropy measures the level of impurity or disorder within a node based on the distribution of class labels. The aim is to minimize entropy by creating splits that result in more homogeneous subsets with respect to class labels.\n",
    "\n",
    "Note that in many cases, Gini impurity and entropy lead to the same results in terms of selecting the best split. The choice between them often comes down to personal preference or computational efficiency. The Gini impurity is slightly faster to compute than entropy, which is why it is the default criterion used in the DecisionTreeClassifier model of scikit-learn.\n",
    "\n",
    "By evaluating the impurity of nodes using these criteria, decision tree algorithms can make informed decisions on how to split the data, resulting in a tree structure that optimally partitions the feature space and minimizes impurity within the resulting subsets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree Classification\n",
    "In regression tree classification, the measurement of impurity or quality of a split is typically done using the Mean Squared Error (MSE). MSE quantifies the average squared difference between the predicted values and the actual values of the target variable.\n",
    "\n",
    "Unlike in classification problems where the impurity measures like Gini impurity or entropy are used, regression trees aim to minimize the MSE to create optimal splits. The goal is to find the splits that result in subsets with the lowest possible MSE, indicating that the predicted values within each subset are close to the mean value of the labels in that leaf.\n",
    "\n",
    "The process of building a regression tree involves recursively partitioning the feature space based on the selected features and split points that minimize the MSE. Each node represents a subset of the data, and the splitting criterion is determined by the MSE reduction achieved by the split.\n",
    "\n",
    "The regression tree algorithm seeks to find the optimal splits that minimize the MSE at each node and create leaves where the predicted target values are as close as possible to the mean value of the labels within that leaf. This way, when new data is passed through the tree, the predicted values are determined by the mean value associated with the leaf node in which they fall.\n",
    "\n",
    "By utilizing the MSE as the impurity measure and creating splits based on minimizing the MSE, regression trees can effectively model the relationships between features and continuous target variables, allowing for accurate predictions based on the mean values within each leaf."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.97\n",
      "Test set RMSE of dt: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split dataset into 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2,stratify=y,random_state=1)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate dt, set 'criterion' to 'gini'\n",
    "dt = DecisionTreeClassifier(criterion= 'gini', random_state=1)\n",
    "\n",
    "# Instantiate dt, set 'entropy' as the information criterion\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=1)\n",
    "\n",
    "# Most of the time, the gini index and entropy lead to the same results.\n",
    "# The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate test-set accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))\n",
    "\n",
    "# REGRESSION\n",
    "# Import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Split data into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor 'dt'\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=3)\n",
    "# 0.1 implies Atleast 10% of the training data\n",
    "\n",
    "# Fit 'dt' to the training-set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test-set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test-set MSE\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test-set RMSE\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - BIAS VARIANCE TRADEOFF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning and the goals\n",
    "In supervised learning, the goal is to find a model, denoted as f1, that approximates the unknown function f as closely as possible. The function f represents the underlying relationship between input variables (x) and output variables (y) in the given dataset.\n",
    "\n",
    "The model f1 can be any algorithm or method chosen for the task, such as Logistic Regression, Decision Trees, Neural Networks, or any other suitable model. The choice of model depends on the nature of the problem, the available data, and the desired properties of the model.\n",
    "\n",
    "During the learning process, the model tries to capture the patterns and relationships present in the training data. However, the training data may contain various types of errors, such as noise or outliers, which can affect the accuracy of the learned model.\n",
    "\n",
    "To handle noise and achieve the goal of minimizing predictive error on unseen datasets, supervised learning algorithms aim to generalize well. Generalization refers to the ability of the learned model to accurately predict outputs for new, unseen data that was not part of the training set.\n",
    "\n",
    "To achieve good generalization, the model should not overly rely on specific details or noise present in the training data. This is often accomplished through techniques such as regularization, cross-validation, or early stopping during the training process. These techniques help prevent overfitting, which occurs when the model becomes too specialized to the training data and performs poorly on new, unseen data.\n",
    "\n",
    "Ultimately, the end goal of supervised learning is to find a model, f1, that achieves a low predictive error on unseen datasets. This allows the model to make accurate predictions on new inputs beyond the training data, effectively capturing the underlying relationship between the input variables and the output variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dificulties in Approximating f and Generalization error\n",
    "When approximating the unknown function f with a model f1, there are several difficulties that can arise:\n",
    "\n",
    "1. Overfitting: Overfitting occurs when the model f1 fits the noise or random fluctuations present in the training data rather than capturing the underlying true pattern. This can lead to poor generalization, where the model performs well on the training data but fails to make accurate predictions on unseen data. Overfitting can be caused by using a model that is too complex or by having insufficient training data.\n",
    "\n",
    "2. Underfitting: Underfitting happens when the model f1 is not flexible enough to capture the complexity of the true function f. It occurs when the model is too simple or lacks the necessary capacity to approximate the underlying relationships in the data. Underfitting can result in high bias, where the model is unable to capture important patterns, leading to poor performance on both the training and unseen data.\n",
    "\n",
    "3. Generalization error: The generalization error measures how well the model f1 performs on unseen data. It quantifies the difference between the predictions made by the model and the true outputs for new inputs. The goal is to minimize the generalization error to ensure that the model can accurately predict outputs for new, unseen data.\n",
    "\n",
    "The generalization error can be decomposed into three components:\n",
    "\n",
    "* Bias: Bias refers to the error introduced by approximating a complex function f with a simpler model f1. If the model has high bias, it may struggle to capture the true underlying pattern, resulting in systematic errors.\n",
    "\n",
    "* Variance: Variance represents the error due to the model's sensitivity to fluctuations in the training data. If the model has high variance, it may fit the training data well but fail to generalize to new data. It tends to be a result of using a model that is too complex or having insufficient training data.\n",
    "\n",
    "* Irreducible error: The irreducible error is the error that cannot be reduced by any model, as it stems from inherent noise or randomness in the data. It represents the minimum achievable error, even with a perfect model.\n",
    "\n",
    "In order to achieve a good approximation of f and minimize the generalization error, it is important to strike a balance between the bias and variance of the model. This can be done by selecting an appropriate model complexity, gathering sufficient and representative training data, and employing regularization techniques to mitigate overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias, Variance, Model Complexity, and Bias Variance Tradeoff\n",
    "\n",
    "The bias-variance tradeoff arises because reducing bias typically increases variance, and reducing variance often increases bias. It is a delicate balancing act to find the optimal model complexity that minimizes both bias and variance, resulting in the best generalization performance.\n",
    "\n",
    "When the model is too simple or has high bias, it may not have enough flexibility to capture the complexities of the underlying function f. This leads to underfitting, where the model performs poorly on both the training data and unseen data.\n",
    "\n",
    "On the other hand, when the model is too complex or has high variance, it becomes overly sensitive to the noise or random fluctuations present in the training data. This can result in overfitting, where the model fits the training data extremely well but fails to generalize to new, unseen data.\n",
    "\n",
    "To strike a balance between bias and variance, it is crucial to tune the model complexity appropriately. This can involve adjusting hyperparameters, such as the maximum tree depth in decision trees or the number of hidden layers and neurons in neural networks. By finding the right level of complexity, we can achieve a model that captures the underlying patterns in the data without overfitting or underfitting.\n",
    "\n",
    "The goal of the bias-variance tradeoff is to minimize the overall generalization error, which includes both bias and variance, allowing the model to make accurate predictions on unseen datasets. This tradeoff is a central challenge in supervised learning, as finding the optimal model complexity requires careful consideration and experimentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Generalization Error, Better Model Evaluation with Cross-Validation, Diagnose Variance Problems, and Diagnose Bias Problems.\n",
    "\n",
    "### Here's a summary of the steps and solutions:\n",
    "1. Split the data: Divide the available data into training and test sets. The training set will be used to train the model, while the test set will be used to evaluate its performance on unseen data.\n",
    "\n",
    "1. Fit f1 to the training set: Train the model f1 using the training data, adjusting the model's parameters or hyperparameters as needed.\n",
    "\n",
    "1. Evaluate the error on the test set: Measure the performance of f1 by calculating the error or other relevant metrics on the test set. This provides an estimate of the generalization error, indicating how well the model is likely to perform on new, unseen data.\n",
    "\n",
    "1. Better evaluation with cross-validation: To improve the evaluation process, it's important to avoid touching the test set until confident about f1's performance. Cross-validation (CV) can be used instead. In k-fold cross-validation, the data is divided into k subsets (folds), and the model is trained and evaluated k times, with each fold serving as the test set once. The average performance across the k iterations provides a more robust estimate of the model's performance.\n",
    "\n",
    "1. Diagnose variance problems: If f1 suffers from high variance, the CV error of f1 will be higher than the training set error of f1. This indicates overfitting, where the model is too complex and sensitive to the training data. To remedy overfitting, you can reduce the model complexity by decreasing parameters like the maximum depth or increasing parameters like the minimum samples per leaf. Another solution is to gather more data to provide a better representation of the underlying patterns.\n",
    "\n",
    "1. Diagnose bias problems: If f1 suffers from high bias, the CV error of f1 will be approximately equal to or significantly higher than the training set error of f1. This suggests underfitting, where the model is too simple and fails to capture the underlying patterns. To address underfitting, you can increase the model complexity by adjusting parameters like the maximum depth or decreasing parameters like the minimum samples per leaf. Additionally, gathering more relevant features can help the model capture more meaningful relationships."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of CARTs\n",
    "CARTs (Classification and Regression Trees) have several limitations, including:\n",
    "\n",
    "1. Orthogonal decision boundaries: CARTs are binary decision trees that split the feature space into rectangular regions. This means they can only produce orthogonal (axis-aligned) decision boundaries. In cases where the decision boundary is nonlinear or requires complex shapes, CARTs may struggle to capture the underlying patterns effectively.\n",
    "\n",
    "2. Sensitivity to training set variations: CARTs are sensitive to small changes or variations in the training set. A slight change in the data can lead to different splits and, consequently, different tree structures. This sensitivity makes CARTs less robust to noise or small fluctuations in the data.\n",
    "\n",
    "3. High variance: Unconstrained CARTs have the tendency to overfit the training set, resulting in high variance. They can learn intricate and detailed decision boundaries that perfectly fit the training data but fail to generalize well to unseen data. This overfitting problem can lead to poor performance on new inputs.\n",
    "\n",
    "To overcome the limitations of CARTs, ensemble learning techniques are often employed. Ensemble learning combines multiple individual models to create a stronger, more accurate prediction. Two popular ensemble methods based on CARTs are:\n",
    "\n",
    "1. Random Forests: Random Forests use an ensemble of multiple decision trees, where each tree is trained on a different subset of the training data and with different random subsets of features. By averaging the predictions of all the trees, Random Forests reduce overfitting and improve generalization performance.\n",
    "\n",
    "2. Boosting algorithms (e.g., AdaBoost, Gradient Boosting): Boosting algorithms train decision trees sequentially, where each subsequent tree focuses on correcting the mistakes made by the previous trees. By iteratively adjusting the weights of the training samples, boosting algorithms create a strong ensemble that can handle complex relationships and reduce bias.\n",
    "\n",
    "Ensemble learning helps mitigate the limitations of individual CARTs by leveraging the collective wisdom of multiple models. It improves the generalization ability, robustness to noise, and overall performance of CART-based models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "he general steps in ensemble learning accurately.\n",
    "\n",
    "1. Training set and different classifiers: The training set is used to train multiple individual models or classifiers. These classifiers can be different algorithms such as decision trees, logistic regression, k-nearest neighbors (KNN), support vector machines (SVM), or others. Each classifier learns its own parameters or weights during training.\n",
    "\n",
    "1. Individual predictions: After training, each classifier independently makes predictions on the input data or unseen examples. Each classifier may have its own strengths, weaknesses, and ways of capturing patterns in the data.\n",
    "\n",
    "1. Meta-model or ensemble model: The predictions of the individual models are then fed into another model, called the meta-model or ensemble model. The meta-model combines or aggregates the predictions from the individual models to generate a final prediction.\n",
    "\n",
    "1. Final prediction: The ensemble model's final prediction is typically more robust and less prone to errors compared to the predictions of any individual model. The ensemble model leverages the diverse perspectives and approaches of the individual models to arrive at a more accurate and reliable prediction.\n",
    "\n",
    "Ensemble learning aims to take advantage of the collective intelligence of multiple models. By combining the predictions of different models, it can mitigate the limitations of individual models and enhance the overall performance. The ensemble model can often achieve better results compared to any single model when the individual models exhibit complementary skills or capture different aspects of the data.\n",
    "\n",
    "The specific techniques used to combine the predictions from the individual models can vary. Common approaches include averaging the predictions, taking a majority vote, using weighted averages, or training a separate model (e.g., stacking or blending) to learn how to combine the predictions effectively.\n",
    "\n",
    "Ensemble learning is a powerful technique that has been successful in various machine learning tasks, improving predictive accuracy, reducing overfitting, and enhancing the generalization ability of the models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE: 0.74\n",
      "Train MSE: 0.73\n",
      "Test MSE: 0.75\n",
      "CV RMSE: 0.86\n",
      "Train RMSE: 0.86\n",
      "\n",
      "Ensemble Learning\n",
      "Logistic Regression : 0.978\n",
      "K Nearest Neighbours : 0.978\n",
      "Classification Tree : 0.956\n",
      "Logistic Regression : 0.978\n",
      "K Nearest Neighbours : 0.978\n",
      "Classification Tree : 0.956\n",
      "Voting Classifier: 0.978\n"
     ]
    }
   ],
   "source": [
    "#K-Fold CV in regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate decision tree regressor and assign it to 'dt'\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=SEED)\n",
    "\n",
    "# Evaluate the list of MSE ontained by 10-fold CV\n",
    "# Set n_jobs to -1 in order to exploit all CPU cores in computation\n",
    "MSE_CV = - cross_val_score(dt, X_train, y_train, cv= 10, scoring= 'neg_mean_squared_error' , n_jobs = -1)\n",
    "\n",
    "# Fit 'dt' to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of training set\n",
    "y_predict_train = dt.predict(X_train)\n",
    "\n",
    "# Predict the labels of test set\n",
    "y_predict_test = dt.predict(X_test)\n",
    "\n",
    "# CV MSE\n",
    "print('CV MSE: {:.2f}'.format(MSE_CV.mean()))\n",
    "\n",
    "# Training set MSE\n",
    "print('Train MSE: {:.2f}'.format(MSE(y_train, y_predict_train)))\n",
    "\n",
    "# Test set MSE\n",
    "print('Test MSE: {:.2f}'.format(MSE(y_test, y_predict_test)))\n",
    "\n",
    "# Suppose CV MSE = 20.51, Train MSE = 15.30 and Test MSE = 20.92\n",
    "# Train MSE < CV MSE.\n",
    "# Suggested that model is overfit and is suffering from high variance.\n",
    "# CV MSE and Test MSE are roughly equal\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV.mean())**(1/2)\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_predict_train))**(1/2)\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
    "\n",
    "# Suppose, RMSE_CV = 5.14, RMSE_train = 5.15 and baseline_RMSE = 5.1\n",
    "# RMSE_CV < RMSE_train means dt suffers from high bias because RMSE_CV ≈ RMSE_train and both scores are greater than baseline_RMSE.\n",
    "# dt is indeed underfitting the training set as the model is too constrained to capture the nonlinear dependencies between features and labels\n",
    "\n",
    "\n",
    "# Ensemble Learning\n",
    "print(\"\\nEnsemble Learning\")\n",
    "\n",
    "# Import functions to compute accuracy and split data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import models, including VotingClassifier meta-model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= SEED)\n",
    "\n",
    "# Instantiate individual classifiers\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "knn = KNN(n_neighbors=27)\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define a list called classifier that contains the tuples (classifier_name, classifier)\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n",
    "\n",
    "# Iterate over the defined list of tuples containing the classifiers\n",
    "for clf_name, clf in classifiers:\n",
    "  \n",
    "  #fit clf to the training set\n",
    "  clf.fit(X_train, y_train)\n",
    "  \n",
    "  # Predict the labels of the test set\n",
    "  y_pred = clf.predict(X_test)\n",
    "  \n",
    "  # Evaluate the accuracy of clf on the test set\n",
    "  print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "  \n",
    "# OR \n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)   \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n",
    "\n",
    "# Instantiate a VotingClassifier 'vc'\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "# Fit 'vc' to the traing set and predict test set labels\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Evaluate the test-set accuracy of 'vc'\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 - BAGGING AND RANDOM FOREST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAGGING\n",
    "Bagging, short for bootstrap aggregating, is an ensemble learning method that aims to improve the performance and stability of machine learning models. It involves training multiple instances of the same algorithm on different subsets of the training data.\n",
    "\n",
    "In bagging, each model is trained independently on a randomly selected subset of the training data, which is created through sampling with replacement. This means that some instances may be sampled multiple times for a single model, while others may not be sampled at all. By using different subsets of the data for training, bagging reduces the variance of individual models in the ensemble.\n",
    "\n",
    "In bagging classification, the predictions of the individual models are aggregated using majority voting. Each model in the ensemble makes its prediction, and the final prediction is determined by the majority vote among the models. This approach is implemented in the BaggingClassifier class in scikit-learn.\n",
    "\n",
    "On the other hand, in bagging regression, the predictions of the individual models are aggregated by averaging. Each model predicts a continuous value, and the final prediction is the average of the predictions made by the models. This technique is implemented in the BaggingRegressor class in scikit-learn.\n",
    "\n",
    "One limitation of bagging is that some instances may be sampled multiple times for one model, while others may not be sampled at all. This can lead to imbalanced representation of instances in the training subsets, potentially affecting the performance of the ensemble. However, bagging is generally effective in reducing overfitting and improving the overall performance and stability of machine learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out Of Bag (OOB) instances\n",
    "In bagging, since each model is trained on a different subset of the training data, there will be some instances that are not included in the training subset for a particular model. These instances are referred to as out-of-bag (OOB) instances.\n",
    "\n",
    "On average, approximately 63% of the training instances are sampled for each model, which means that around 37% of the instances are OOB instances. These OOB instances are not seen by the model during training, as they were not part of the training subset for that specific model.\n",
    "\n",
    "The OOB instances can be used to estimate the performance of the model without the need for additional cross-validation techniques. After training the ensemble of models, each instance can be passed through the models for which it was an OOB instance, and the predictions can be aggregated or evaluated individually. This provides an estimate of the model's performance on unseen data.\n",
    "\n",
    "This technique is known as OOB evaluation and is a useful tool for assessing the performance of bagging models without the need for additional data splitting or cross-validation procedures. It can provide a reliable estimate of the model's generalization performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random Forest is another popular ensemble learning method that builds upon the concept of bagging. It uses a collection of decision trees as base estimators and introduces additional randomization to enhance the diversity and robustness of the ensemble.\n",
    "\n",
    "In Random Forest, each decision tree in the ensemble is trained on a different bootstrap sample, which is created by randomly sampling the training data with replacement. Each tree is trained on a subset of the data with the same size as the original training set. This helps introduce diversity in the training process.\n",
    "\n",
    "Furthermore, Random Forest introduces random feature selection at each node of the decision trees. Instead of considering all the features at each node, a subset of features (d) is randomly selected without replacement. This further enhances the diversity of the individual trees and prevents the dominance of any single feature.\n",
    "\n",
    "In Random Forest classification, the predictions of the individual trees are aggregated using majority voting. Each tree makes its prediction, and the final prediction is determined by the majority vote among the trees. The RandomForestClassifier class in scikit-learn provides an implementation of Random Forest for classification tasks.\n",
    "\n",
    "In Random Forest regression, the predictions of the individual trees are aggregated through averaging. Each tree predicts a continuous value, and the final prediction is the average of the predictions made by the trees. The RandomForestRegressor class in scikit-learn provides an implementation of Random Forest for regression tasks.\n",
    "\n",
    "Random Forests are known for their ability to handle high-dimensional datasets, handle non-linear relationships, and provide good generalization performance. The randomization introduced in the training process helps reduce overfitting and improves the overall performance and robustness of the ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Tree-based methods, including Random Forest, allow for the measurement of feature importance, which indicates the relative significance of each feature in making predictions. In scikit-learn, the importance of each feature can be accessed using the attribute feature_importances_.\n",
    "\n",
    "The feature importance provided by tree-based methods is typically calculated based on how much each feature contributes to reducing impurity (e.g., Gini impurity or entropy) in the decision tree or ensemble. The importance of a feature is calculated as a weighted average of the impurity reductions caused by that feature over all the nodes in the tree or ensemble.\n",
    "\n",
    "By accessing the feature_importances_ attribute of a trained tree-based model in scikit-learn, you can obtain an array or a list that represents the importance scores of each feature. The higher the score, the more important the feature is considered in the prediction process.\n",
    "\n",
    "For example, if you have a trained Random Forest classifier named rf_classifier, you can access the feature importance using rf_classifier.feature_importances_. Similarly, for a decision tree classifier named dt_classifier, you can use dt_classifier.feature_importances_.\n",
    "\n",
    "Feature importance can be useful for various tasks such as feature selection, understanding the underlying relationships in the data, identifying the most influential factors, and interpreting the model's behavior. It provides insights into which features have the most predictive power and can guide further analysis or decision-making processes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\app\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier: 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\app\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.944\n",
      "OOB accuracy: 0.919\n",
      "Test set RMSE of rf: 0.85\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BAGGING CLASSIFICATION\n",
    "# Import models and utility functions\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_wine()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "\n",
    "# Fit 'bc' to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate and print test-set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))\n",
    "\n",
    "\n",
    "# OOB EVALUATION IN SKLEARN\n",
    "# Import models and split utility function\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, stratify= y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'; set oob_score= True\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "# Fit 'bc' to the traing set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Extract the OOB accuracy from 'bc'\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "# Print test set accuracy\n",
    "print('Test set accuracy: {:.3f}'.format(test_accuracy))\n",
    "\n",
    "# Print OOB accuracy\n",
    "print('OOB accuracy: {:.3f}'.format(oob_accuracy))\n",
    "# The difference between test and oob accuracy will be minimal which proved that we don't need cross validation to check the model accuracy\n",
    "\n",
    "\n",
    "# RANDOM FOREST REGRESSOR\n",
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a random forests regressor 'rf' 400 estimators\n",
    "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "# Fit 'rf' to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "\n",
    "\n",
    "# FEATURE IMPORTANCE in sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances_rf = pd.Series(rf.feature_importances_, index=housing.feature_names)\n",
    "\n",
    "# Sort importances_rf\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "# Make a horizontal bar plot\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 BOOSTING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble method in which multiple models, often referred to as weak learners, are trained sequentially. Each model in the ensemble learns from the errors made by its predecessors, with the goal of improving the overall predictive performance.\n",
    "\n",
    "Boosting combines several weak learners to create a strong learner that performs better than any individual weak learner. A weak learner is a model that performs slightly better than random guessing. An example of a weak learner is a decision stump, which is a decision tree with a maximum depth of 1. A decision stump makes predictions based on a single feature and a threshold.\n",
    "\n",
    "The training process of boosting involves sequentially training the weak learners. Each subsequent weak learner focuses on correcting the mistakes made by the previous models in the ensemble. The training algorithm assigns higher weights to the instances that were incorrectly predicted by the previous models, so that the subsequent models pay more attention to those instances.\n",
    "\n",
    "Two popular boosting algorithms are AdaBoost (Adaptive Boosting) and Gradient Boosting. AdaBoost assigns weights to the training instances and adjusts them during the training process to emphasize the difficult-to-classify instances. Gradient Boosting, on the other hand, fits the weak learners to the residuals or errors made by the previous models, gradually reducing the errors and improving the predictions.\n",
    "\n",
    "By combining the predictions of multiple weak learners, boosting can create a powerful ensemble model that achieves high accuracy and generalization performance. Boosting is particularly effective in handling complex tasks and datasets with high variance or noisy data.\n",
    "\n",
    "It's worth noting that scikit-learn provides implementations of AdaBoost and Gradient Boosting algorithms, namely AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, and GradientBoostingRegressor, which can be used to apply boosting techniques to your machine learning tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost stands for Adaptive Boosting and is a popular boosting algorithm. It is an ensemble method that sequentially trains a series of weak learners, with each learner paying more attention to the instances that were incorrectly predicted by its predecessor.\n",
    "\n",
    "The adaptation in AdaBoost is achieved by changing the weights of the training instances. Initially, all instances are given equal weights, and each weak learner is trained on the weighted data. After each iteration, the weights of the instances that were incorrectly predicted are increased, making them more influential in the subsequent iterations. This process focuses the subsequent weak learners on the difficult-to-classify instances.\n",
    "\n",
    "Each predictor in AdaBoost is assigned a coefficient, denoted as α, which depends on the predictor's training error. Predictors with lower training errors are assigned higher coefficients, indicating their higher importance in the ensemble.\n",
    "\n",
    "AdaBoost classification combines the predictions of multiple weak learners using weighted majority voting. Each weak learner's prediction is weighted by its corresponding α coefficient, and the final prediction is determined by the weighted majority vote.\n",
    "\n",
    "In scikit-learn, the AdaBoostClassifier class is provided for AdaBoost classification tasks. It allows you to specify the base estimator (weak learner), the number of estimators (weak learners), and the learning rate (η). The learning rate is a hyperparameter that controls the contribution of each weak learner to the ensemble. A smaller learning rate shrinks the α coefficients, leading to a more conservative ensemble. It is a tradeoff between the learning rate and the number of estimators, as a smaller learning rate may require a higher number of estimators to achieve good performance.\n",
    "\n",
    "AdaBoost regression, on the other hand, combines the predictions of weak learners using weighted averaging. The final prediction is calculated as the weighted average of the predictions made by each weak learner.\n",
    "\n",
    "In scikit-learn, the AdaBoostRegressor class is available for AdaBoost regression tasks. It shares similar parameters as the AdaBoostClassifier, allowing you to customize the base estimator, number of estimators, and learning rate.\n",
    "\n",
    "AdaBoost is a powerful algorithm that has proven to be effective in various machine learning tasks. It can handle both classification and regression problems and has good generalization performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees is another boosting algorithm that sequentially corrects the errors made by its predecessors. However, unlike AdaBoost, it does not tweak the weights of the training instances. Instead, each predictor in Gradient Boosted Trees is trained using the residual errors (the differences between the true labels and the predictions of the previous models) as the new labels. This allows each subsequent predictor to focus on the remaining errors and improve upon the ensemble's predictions.\n",
    "\n",
    "In Gradient Boosted Trees, a CART (Classification and Regression Tree) is commonly used as the base learner or weak learner. The base learner is typically a decision tree with a maximum depth greater than 1, allowing for more complex models to be learned.\n",
    "\n",
    "Gradient Boosted Regression is a variation of Gradient Boosting that is used for regression problems. The predicted value y is updated in each iteration by adding the learning rate (η) multiplied by the residual error. The process continues with each subsequent predictor correcting the errors made by its predecessors. In scikit-learn, you can use the GradientBoostingRegressor class for Gradient Boosted Regression tasks.\n",
    "\n",
    "Gradient Boosted Classification, on the other hand, is used for classification problems. It follows a similar principle as Gradient Boosted Regression, but the final predictions are obtained through a probability-based approach. In scikit-learn, the GradientBoostingClassifier class can be used for Gradient Boosted Classification.\n",
    "\n",
    "While Gradient Boosting is a powerful technique, it does have some drawbacks. One of the main limitations is that it involves an exhaustive search procedure during the training of each CART. This search aims to find the best split points and features, but it can be computationally expensive. Additionally, in some cases, the CARTs may end up using the same split points and possibly the same features, leading to less diversity within the ensemble.\n",
    "\n",
    "To address the computational cost, Stochastic Gradient Boosting introduces randomness to the training process. Each tree is trained on a random subset of the training data, typically sampling 40% to 80% of the instances without replacement. Additionally, when choosing split points, features are also sampled without replacement. This randomness adds further diversity to the ensemble and helps avoid overfitting. However, it also introduces additional variance to the ensemble of trees."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\app\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.99\n",
      "Test set RMSE: 0.62\n",
      "Test set RMSE: 0.66\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classication in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "# Instantiate an AdaBoost classifier 'adab_clf'\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "# Fit 'adb_clf' to the training set\n",
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set probabilities of positive class\n",
    "# Once the classifier adb_clf is trained, call the .predict_proba() method by passing X_test as a parameter \n",
    "# Extract these probabilities by slicing all the values in the second column as follows\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "\n",
    "\n",
    "adb_clf_roc_auc_score= roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print adb_clf_roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))\n",
    "\n",
    "\n",
    "# Gradient Boosting in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate a GradientBoostingRegressor 'gbt'\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
    "\n",
    "# Fit 'gbt' to the training set\n",
    "gbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))\n",
    "\n",
    "\n",
    "# Stochastic Gradient Boosting in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate a stochastic GradientBoostingRegressor 'sgbt'\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=SEED)\n",
    "# 0.8 refers to sample 80% of datafor training\n",
    "# 0.2 refers to each tree uses 20% of the available features to perform best split\n",
    "\n",
    "# Fit 'sgbt' to the training set\n",
    "sgbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "# Evaluate test set RMSE 'rmse_test'\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print 'rmse_test'\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Model Tuning\n",
    "* The hyperparameters of a machine learning model are parameters that are not learned from data.\n",
    "* They should be set prior to fitting the model to the training set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Hyperparameters\n",
    "Hyperparameters in machine learning models are parameters that are not learned from the data but are set prior to training the model. These hyperparameters control the behavior and performance of the model during the learning process. Some common examples of hyperparameters include the learning rate, regularization strength, number of hidden units in a neural network, maximum tree depth in decision trees, and so on.\n",
    "\n",
    "Here's a breakdown of the differences between parameters and hyperparameters:\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* Learned from data during the training process.\n",
    "* Adjusted by the model to optimize its performance on the training set.\n",
    "* Represent the internal variables or weights of the model.\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "* Not learned from data but set by the user or researcher.\n",
    "* Control the behavior and performance of the model during training.\n",
    "* Often determined through trial and error or using techniques like grid search or random search.\n",
    "* Examples include the number of hidden layers in a neural network, the learning rate of an optimizer, the regularization strength in linear models, and so on.\n",
    "\n",
    "In the case of a CART (Classification and Regression Trees) model, some hyperparameters could be:\n",
    "\n",
    "1. max_depth: The maximum depth of the decision tree. It limits the number of levels or splits the tree can have.\n",
    "\n",
    "1. min_samples_leaf: The minimum number of samples required to be in a leaf node. It controls the minimum size of the leaf nodes, preventing further splitting if the number of samples falls below this threshold.\n",
    "\n",
    "1. splitting criterion: The criterion used for splitting nodes in the tree. For example, it could be Gini impurity or information gain for classification tasks, or mean squared error for regression tasks.\n",
    "\n",
    "Tuning these hyperparameters properly is crucial for achieving good model performance. It often involves experimentation and validation using techniques like cross-validation to find the best combination of hyperparameters that generalizes well to unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is hyperparameter tuning?\n",
    "Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a learning algorithm that results in the best-performing model. The goal is to search through different combinations of hyperparameters and evaluate their impact on the model's performance.\n",
    "\n",
    "The process of hyperparameter tuning involves the following steps:\n",
    "\n",
    "1. Define a search space: Determine the range or possible values for each hyperparameter that will be explored during the tuning process. This could be a discrete set of values or a continuous range.\n",
    "\n",
    "1. elect an optimization strategy: Choose a method to search through the defined search space and find the optimal combination of hyperparameters. Common strategies include grid search, random search, Bayesian optimization, or genetic algorithms.\n",
    "\n",
    "1. Evaluate performance: Use a performance metric to assess the quality of each model trained with different hyperparameter values. The choice of metric depends on the specific problem. In scikit-learn, the default metrics for classification tasks are accuracy, precision, recall, F1-score, etc., while for regression tasks, it is often R-squared, mean squared error (MSE), mean absolute error (MAE), etc.\n",
    "\n",
    "1. Cross-validation: To estimate the generalization performance of each model, cross-validation is typically employed. This involves dividing the training data into multiple folds, training the model on a subset of folds, and evaluating its performance on the remaining fold. This process is repeated for each combination of hyperparameters, and the average performance across folds is used to determine the model's quality.\n",
    "\n",
    "1. Select the best hyperparameters: Once the performance of different hyperparameter combinations has been evaluated, the set of hyperparameters that resulted in the highest performance score is selected as the optimal set.\n",
    "\n",
    "Hyperparameter tuning is an essential step in machine learning model development as it allows you to find the best configuration for your model, improving its performance and generalization to unseen data. It helps in avoiding overfitting or underfitting and finding the right balance between model complexity and simplicity.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why tune hyperparameters?\n",
    "\n",
    "Hyperparameter tuning is necessary because the default hyperparameters provided by scikit-learn (or any other machine learning library) may not be optimal for all problems or datasets. These default values are often chosen as reasonable starting points, but they may not result in the best model performance for a specific task.\n",
    "\n",
    "Here are a few reasons why hyperparameter tuning is important:\n",
    "\n",
    "1. Performance improvement: The choice of hyperparameters can significantly impact the performance of a machine learning model. By tuning the hyperparameters, you have the opportunity to find the configuration that leads to the best possible model performance. This can result in higher accuracy, better predictive power, or improved generalization to unseen data.\n",
    "\n",
    "2. Overfitting and underfitting avoidance: Hyperparameters play a crucial role in controlling the complexity of a model. If the hyperparameters are not appropriately set, the model may suffer from overfitting, where it memorizes the training data too well and fails to generalize to new data. On the other hand, underfitting occurs when the model is too simple to capture the underlying patterns in the data. Tuning the hyperparameters helps strike the right balance between model complexity and generalization ability.\n",
    "\n",
    "3. Dataset characteristics: Different datasets have unique characteristics, such as varying feature scales, dimensions, noise levels, or class imbalances. Hyperparameter tuning allows you to adapt the model to these specific characteristics, ensuring optimal performance on the given dataset.\n",
    "\n",
    "4. Algorithm-specific considerations: Different machine learning algorithms have their own hyperparameters that control their behavior. Tuning these algorithm-specific hyperparameters is crucial to leveraging the strengths of the algorithm and improving its performance for a specific problem.\n",
    "\n",
    "By tuning the hyperparameters, you can obtain the best possible model performance for your specific problem, ensuring that your model is well-suited to the data and maximizing its predictive capabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to hyperparameter tuning\n",
    "There are several approaches to hyperparameter tuning, and some of the commonly used ones include:\n",
    "\n",
    "1. Grid Search: Grid search involves specifying a grid of hyperparameter values to explore exhaustively. The algorithm then evaluates the model performance for each combination of hyperparameters using cross-validation. Grid search is simple to implement and guarantees that all possible combinations are tested, but it can be computationally expensive, especially when dealing with a large number of hyperparameters or a wide range of values.\n",
    "\n",
    "2. Random Search: Random search randomly samples from the defined search space of hyperparameters. Unlike grid search, it does not systematically explore all possible combinations but rather randomly selects combinations for evaluation. Random search is more efficient than grid search in terms of computational resources since it does not evaluate all combinations. It also has the advantage of potentially exploring less common combinations that might yield good results.\n",
    "\n",
    "3. Bayesian Optimization: Bayesian optimization uses probabilistic models to model the performance of the model as a function of the hyperparameters. It iteratively selects the next set of hyperparameters to evaluate based on the expected improvement from previous evaluations. Bayesian optimization is an adaptive method that tends to converge quickly to good hyperparameter values and is suitable when the search space is large or continuous. It requires fewer iterations than grid search or random search but can be more computationally demanding.\n",
    "\n",
    "4. Genetic Algorithms: Genetic algorithms are inspired by natural selection and evolutionary principles. It involves maintaining a population of potential solutions (sets of hyperparameters) and applying genetic operators such as mutation, crossover, and selection to evolve the population over generations. The performance of each set of hyperparameters is evaluated, and the fittest individuals are selected to produce the next generation. Genetic algorithms can explore the search space efficiently, especially when the search space is large or discrete.\n",
    "\n",
    "These are just a few examples of hyperparameter tuning approaches, and there are other techniques available as well, such as gradient-based optimization, tree-based methods, and more. The choice of the tuning approach depends on various factors, including the search space, computational resources, and the nature of the problem at hand. It is often recommended to combine multiple techniques and experiment with different approaches to find the best hyperparameter values for a given model and dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search cross validation\n",
    "Grid search cross-validation is an approach to hyperparameter tuning where a grid of discrete hyperparameter values is manually defined. It exhaustively searches through this grid to find the optimal combination of hyperparameters that results in the best cross-validation (CV) score.\n",
    "\n",
    "Here's an example of how grid search cross-validation works:\n",
    "\n",
    "1. Hyperparameters grids: Define a set of hyperparameters and their corresponding values to be explored. In this example, let's consider two hyperparameters for tuning a Random Forest (RF) model: max_depth and min_samples_leaf.\n",
    "\n",
    "   - max_depth: Possible values are {2, 3, 4}.\n",
    "   - min_samples_leaf: Possible values are {0.05, 0.1}.\n",
    "\n",
    "2. hyperparameter space: Create a grid by combining all possible values from the hyperparameters. In this case, the hyperparameter space would consist of six combinations:\n",
    "   - (2, 0.05)\n",
    "   - (2, 0.1)\n",
    "   - (3, 0.05)\n",
    "   - (3, 0.1)\n",
    "   - (4, 0.05)\n",
    "   - (4, 0.1)\n",
    "\n",
    "\n",
    "3. CV scores: For each set of hyperparameters in the grid, train a RF model using those hyperparameters and evaluate its performance using cross-validation. Calculate the CV score for each model. The CV score is typically based on a chosen metric for model performance, such as accuracy or R-squared.\n",
    "\n",
    "4. optimal hyperparameters: Identify the set of hyperparameters that corresponds to the model achieving the best CV score. These optimal hyperparameters are the ones that yield the highest performance based on the chosen metric.\n",
    "\n",
    "Tuning hyperparameters using grid search cross-validation can be computationally expensive, especially when the search space is large or the model training process is time-consuming. However, it ensures an exhaustive search through all possible combinations and guarantees finding the best hyperparameter values within the defined grid.\n",
    "\n",
    "It's important to note that the number of combinations grows exponentially as more hyperparameters and values are added to the grid, which can make grid search impractical for large hyperparameter spaces. In such cases, other techniques like random search or Bayesian optimization can be considered as more efficient alternatives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "You are correct in highlighting some important aspects of hyperparameter tuning:\n",
    "\n",
    "1. Computationally expensive: Hyperparameter tuning can be computationally expensive, especially when the search space is large, and training the model is time-consuming. Grid search, for example, involves evaluating the model for every combination of hyperparameters in the defined grid, which can be time and resource-intensive. It's important to consider the trade-off between the computational cost and the potential improvement in model performance when deciding on the extent of hyperparameter tuning.\n",
    "\n",
    "2. Slight improvement: Tuning hyperparameters does not always guarantee a significant improvement in model performance. Sometimes, even after extensive tuning, the improvement may be marginal. It is important to have realistic expectations and understand that hyperparameter tuning is not a magic bullet that will dramatically transform a poorly performing model into a highly accurate one. The impact of tuning can vary depending on the specific problem, dataset, and algorithm being used.\n",
    "\n",
    "3. Weighing the impact on the project: When deciding how much effort to dedicate to hyperparameter tuning, it's crucial to consider the overall impact on the project. You need to evaluate the potential benefits of fine-tuning the model against the costs associated with additional computational resources, time, and effort. If the project has strict time constraints or the marginal improvement from tuning is negligible, it may be more practical to settle for a reasonably performing model with default hyperparameters.\n",
    "\n",
    "It's worth noting that hyperparameter tuning should be approached with a balanced perspective. While it can lead to performance improvements, it should be carefully managed to avoid excessive time and resource consumption. It's important to strike a balance between the potential gains from tuning and the practical considerations of the project, such as deadlines, resource limitations, and the significance of the improvement relative to the project's goals."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}\n",
      "Best hyerparameters:\n",
      " {'max_depth': 3, 'max_features': 0.4, 'min_samples_leaf': 0.06}\n",
      "Best CV accuracy\n",
      "Test set accuracy of best model: 0.942\n",
      "Test set ROC AUC score: 0.963\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best hyerparameters:\n",
      " {'max_depth': 4, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'n_estimators': 500}\n",
      "Test set RMSE of rf: 0.28\n"
     ]
    }
   ],
   "source": [
    "#Inspecting the hyperparameters of a CART\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Set seed to 1 for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt'\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# Print out 'dt's hyperparameters\n",
    "print(dt.get_params())\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Define the grid of hyperparameters 'params_dt'\n",
    "params_dt = {\n",
    "              'max_depth': [3, 4,5, 6],\n",
    "              'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "              'max_features': [0.2, 0.4,0.6, 0.8]\n",
    "            }\n",
    "\n",
    "# Instantiate a 10-fold CV grid search object 'grid_dt'\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "\n",
    "# Fit 'grid_dt' to the training data\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Extract best hyperparameters from 'grid_dt'\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print('Best hyerparameters:\\n', best_hyperparams)\n",
    "\n",
    "# Extract best CV score from 'grid_dt'\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print('Best CV accuracy'.format(best_CV_score))\n",
    "\n",
    "# Extract best model from 'grid_dt'\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "test_acc = best_model.score(X_test,y_test)\n",
    "\n",
    "# Print test set accuracy\n",
    "print(\"Test set accuracy of best model: {:.3f}\".format(test_acc))\n",
    "\n",
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))\n",
    "\n",
    "\n",
    "# Inspecting RF Hyperparameters in sklearn\n",
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=3)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate a random forests regressor 'rf'\n",
    "rf = RandomForestRegressor(random_state= SEED)\n",
    "\n",
    "# Inspect rf' s hyperparameters\n",
    "rf.get_params()\n",
    "\n",
    "# Basic imports\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load data\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=3)\n",
    "\n",
    "# Define a grid of hyperparameter 'params_rf'\n",
    "params_rf = {\n",
    "                'n_estimators': [300, 400, 500],\n",
    "                'max_depth': [4, 6, 8],\n",
    "                'min_samples_leaf': [0.1, 0.2],\n",
    "                'max_features': ['log2','sqrt']\n",
    "            }\n",
    "\n",
    "# Instantiate 'grid_rf'\n",
    "grid_rf = GridSearchCV(estimator=rf,param_grid=params_rf, cv=3, scoring= 'neg_mean_squared_error',verbose=1, n_jobs=-1)\n",
    "\n",
    "# Searching for the best hyperparameters\n",
    "# Fit 'grid_rf' to the training set\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract best hyperparameters from 'grid_rf'\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "print('Best hyerparameters:\\n', best_hyperparams)\n",
    "\n",
    "# Extract best model from 'grid_rf'\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
