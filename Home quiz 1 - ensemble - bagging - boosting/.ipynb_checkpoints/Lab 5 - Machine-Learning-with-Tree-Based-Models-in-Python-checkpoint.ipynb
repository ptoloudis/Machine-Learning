{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based-Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "In this home quiz you must do the following:\n",
    "\n",
    "1) Study the 5 presentations about three models and their performance. I will ask each group to make a presentation of each part. You can enhance the presentation using web resources or the material that I will upload today in piaza.\n",
    "\n",
    "2) For each part there is a code. You must select appropriate dataset to run it. You will use the codes to support your presentations.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - CLASSIFICATION AND REGRESSION TREES (CART)\n",
    "- Set of supervised learning models used for problems involving classification and regression\n",
    "\n",
    "## Classification Tree\n",
    "- Sequence of if-else questions about individual features\n",
    "- Objective: Infer class labels\n",
    "- Able to capture non-linear relationships between features and labels\n",
    "- Don't require feature scaling. For example, it do not need standardization etc.\n",
    "\n",
    "## Decision regions \n",
    "- Decision region is the region in the feature space where all the instances are assigned to one class label.\n",
    "- For example, if result is of two class Pass or Fail. Then there will be 2 decision region. One is Pass region other is Fail region.\n",
    "\n",
    "## Decision boundary\n",
    "- It is the seperating boundary between two region.\n",
    "- In above example, decision boundary will be 33% (which is the passing marks)\n",
    "\n",
    "## Logistic regression vs classification tree\n",
    "- A classification tree divides the feature space into rectangular regions.\n",
    "- In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions.\n",
    "- In other word, decision boundary produced by logistic regression is linear (line) while the boundaries produced by the classification tree divide the feature space into rectangular regions (Not a line but boxes/region it divides two class).\n",
    "\n",
    "## Building block of Decision Tree \n",
    "- Root: No parent node, question giving rise to two children nodes.\n",
    "- Internal node: One parent node, question giving rise to two children nodes.\n",
    "- Lead: One parent node, no children node -> Prediction.\n",
    "\n",
    "## Classication-Tree Learning (Working) - \n",
    "- Nodes are grown recursively (based on last node).\n",
    "- At each node, split the data based on:\n",
    "1. feature f and split-point(sp) to maximize IG(Information gain from each node).\n",
    "2. If IG(node)= 0, declare the node a leaf.\n",
    "\n",
    "## Information Gain-\n",
    "- IG is a synonym for Kullback–Leibler divergence.\n",
    "- It is the amount of information gained about a random variable or signal from observing another random variable.\n",
    "- The term is sometimes used synonymously with mutual information, which is the conditional expected value of the Kullback–Leibler divergence.\n",
    "- KL divergance is the univariate probability distribution of one variable from the conditional distribution of this variable given the other one.\n",
    "\n",
    "## Criteria to measure the impurity of a node I(node):\n",
    "1. Variance (Regression) [Variance reduction of a node N is defined as the total reduction of the variance of the target variable x due to the split at this node]\n",
    "2. Gini impurity (Classification) [Measure of impurity. Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset]\n",
    "3. Entropy (Classification) [Measure of purity. Information entropy is the average rate at which information is produced by a stochastic source of data]\n",
    "\n",
    "Note \n",
    "- Most of the time, the gini index and entropy lead to the same results.\n",
    "- The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn\n",
    "\n",
    "## Regression Tree Classification\n",
    "- Measurement are done through MSE (Mean Square error)\n",
    "- Information Gain is the MSE. So the target variable will have the Mean Square Error.\n",
    "- Regression trees tries to find the split that produce the leaf where in each leaf, the target value are an average of closest possible to the mean value of labels in that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - BIAS VARIANCE TRADEOFF\n",
    "\n",
    "## Supervised Learning\n",
    "- y = f(x), f is the function which is unknown\n",
    "- Our model output will be that function\n",
    "- But that function may contains various type of error like noise\n",
    "\n",
    "## Goals of Supervised Learning\n",
    "- Find a model f1 that best approximates f: f1 ≈ f ()\n",
    "- f1 can be LogisticRegression, Decision Tree, Neural Network ...\n",
    "- Discard noise as much as possible.\n",
    "- End goal:f1 should acheive a low predictive error on unseen datasets.\n",
    "\n",
    "## Dificulties in Approximating f\n",
    "- Overtting: f1(x) fits the training set noise.\n",
    "- Undertting: f1 is not flexible enough to approximate f\n",
    "\n",
    "## Generalization error \n",
    "- Generalization Error of f1 : Does f1 generalize well on unseen data?\n",
    "- It can be decomposed as follows: Generalization Error of\n",
    "- f1 = bias + variance + irreducible error\n",
    "\n",
    "## Bias\n",
    "- Bias: error term that tells you, on average, how much f1 ≠ f.\n",
    "- High Bias lead to underfitting\n",
    "\n",
    "## Variance\n",
    "- Variance: tells you how much f is inconsistent over different training sets.\n",
    "- High Variance lead to overfitting\n",
    "\n",
    "- If we decrease Bias then Variance increase. Or Vice versa.\n",
    "\n",
    "## Model Complexity\n",
    "- Model Complexity: sets the flexibility of f1.\n",
    "- Example: Maximum tree depth, Minimum samples per leaf etc etc.\n",
    "\n",
    "## Bias Variance Tradeoff \n",
    "- It is the problem is in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.\n",
    "\n",
    "## Estimating the Generalization Error, Solution:\n",
    "- Split the data to training and test sets \n",
    "- Fit t1 to the training set\n",
    "- Evaluate the error of f1 on the unseen test set\n",
    "- Generalization error of f1 ≈ test set error of f1.\n",
    "\n",
    "## Better Model Evaluation with Cross-Validation\n",
    "- Test set should not be touched until we are confident about f1's performance.\n",
    "- Evaluating f1 on training set: biased estimate,f1 has already seen all training points.\n",
    "- Solution → K Cross-Validation (CV)\n",
    "\n",
    "## Diagnose Variance Problems\n",
    "- If f1 suffers from high variance: CV error of f1 > training set error of f1.\n",
    "- f1 is said to overfit the training set. To remedy overtting:\n",
    "- decrease model complexity\n",
    "- for ex: decrease max depth, increase min samples per leaf\n",
    "- gather more data\n",
    "\n",
    "## Diagnose Bias Problems\n",
    "- If f1 suffers from high bias: CV error of f1 ≈ training set error of f1 >> desired error.\n",
    "- f1 is said to underfit the training set. To remedy underfitting:\n",
    "- increase model complexity\n",
    "- for ex: increase max depth, decrease min samples per leaf\n",
    "- gather more relevant features\n",
    "\n",
    "## Limitations of CARTs\n",
    "- Classification: can only produce orthogonal decision boundaries.\n",
    "- Sensitive to small variations in the training set.\n",
    "- High variance: unconstrained CARTs may overt the training set.\n",
    "- Solution: ensemble learning.\n",
    "\n",
    "## Ensemble Learning\n",
    "- Train different models on the same dataset.\n",
    "- Let each model make its predictions.\n",
    "- Meta-model: aggregates predictions of individual models.\n",
    "- Final prediction: more robust and less prone to errors.\n",
    "- Best results: models are skillful in different ways.\n",
    "\n",
    "## Steps in Ensemble learning \n",
    "1. Training set is fed to different classifier like Decision tree, Logistic regression, KNN etc.\n",
    "2. Each classifier learn its parameter and make prediction\n",
    "3. Each prediction are fed into another model and that model make final prediction.\n",
    "4. That final model is known as ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 - BAGGING AND RANDOM FOREST\n",
    "\n",
    "## Bagging\n",
    "- Bagging is an ensemble method involving training the same algorithm many times using different subsets sampled from the training data\n",
    "- In bagging, it uses same algorithm (only one algo is used)\n",
    "- However the model is not training on entire training set\n",
    "- Instead each model is trained on different subset of data\n",
    "- Bagging: Bootstrap Aggregation.\n",
    "- Uses a technique known as the bootstrap.\n",
    "- Reduces variance of individual models in the ensemble.\n",
    "- For example, suppose a training dataset contains 3 parts - a,b,c.\n",
    "- It create subset by method sample by replacement. For example aaa,aab,aba,acc,aca etc.\n",
    "- On this subset, the models are trained.\n",
    "\n",
    "## Bagging Classication\n",
    "- Aggregates predictions by majority voting (Final model is selected by voting).\n",
    "- BaggingClassifier in scikit-learn.\n",
    "\n",
    "## Bagging Regression\n",
    "- Aggregates predictions through averaging (Final model is selected by avergaing).\n",
    "- BaggingRegressor in scikit-learn.\n",
    "\n",
    "## Bagging limitations\n",
    "- Some instances may be sampled severaltimes for one model,\n",
    "- Other instances may not be sampled at all.\n",
    "\n",
    "## Out Of Bag (OOB) instances\n",
    "- On average,for each model, 63% ofthe training instances are sampled.\n",
    "- The remaining 37% constitute the OOB instances.\n",
    "- Since OOB instances are not seen by the model during training.\n",
    "- This can be used to estimate the performance of the model without the need of cross validation.\n",
    "- This technique is known as OOB evaluation\n",
    "\n",
    "## Random Forest\n",
    "- Another ensemble model\n",
    "- Base estimator: Decision Tree\n",
    "- Each estimator is trained on a different bootstrap sample having the same size as the training set\n",
    "- RF introduces further randomization in the training of individual trees\n",
    "- d features are sampled at each node without replacement ( d < total number of features)\n",
    "\n",
    "## Random Forests Classication:\n",
    "- Aggregates predictions by majority voting\n",
    "- RandomForestClassifier in scikit-learn\n",
    "\n",
    "## Random Forests Regression:\n",
    "- Aggregates predictions through averaging\n",
    "- RandomForestRegressor in scikit-learn\n",
    "\n",
    "## Feature Importance\n",
    "- Tree-based methods: enable measuring the importance of each feature in prediction.\n",
    "- In sklearn :\n",
    "- how much the tree nodes use a particular feature (weighted average) to reduce impurity\n",
    "- accessed using the attribute feature_importance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 BOOSTING\n",
    "\n",
    "- Boosting refers to an ensemble method in which several models are trained sequentially with each model learning from the errors of its predecessors. \n",
    "- Boosting: Ensemble method combining several weak learners to form a strong learner.\n",
    "- Weak learner: Model doing slightly better than random guessing.\n",
    "- Example of weak learner: Decision stump (CART whose maximum depth is 1).\n",
    "- Train an ensemble of predictors sequentially.\n",
    "- Each predictor tries to correct its predecessor.\n",
    "- Most popular boosting methods: AdaBoost, Gradient Boosting.\n",
    "\n",
    "## Adaboost\n",
    "- Stands for Adaptive Boosting.\n",
    "- Each predictor pays more attention to the instances wrongly predicted by its predecessor.\n",
    "- Achieved by changing the weights of training instances.\n",
    "- Each predictor is assigned a coefficient α.\n",
    "- α depends on the predictor's training error\n",
    "- Learning rate: 0 < η ≤ 1. It help to shrink coeeficient α. It is the tradeoff between η and number of estimator.\n",
    "- Smaller number of η should be compensiated by high number of estimator.\n",
    "\n",
    "## AdaBoost Classication\n",
    "- Weighted majority voting.\n",
    "- In sklearn: AdaBoostClassifier.\n",
    "\n",
    "## AdaBoost Regression\n",
    "- Weighted average.\n",
    "- In sklearn: AdaBoostRegressor.\n",
    "\n",
    "## Gradient Boosted Trees\n",
    "- Sequential correction of predecessor's errors.\n",
    "- Does not tweak the weights of training instances.\n",
    "- Fit each predictor is trained using its predecessor's residual errors as labels.\n",
    "- Gradient Boosted Trees: a CART is used as a base learner.\n",
    "\n",
    "## Gradient Boosted Regression:\n",
    "- y = y + ηr + ... + ηr\n",
    "- In sklearn: GradientBoostingRegressor .\n",
    "\n",
    "## Gradient Boosted Classication:\n",
    "- In sklearn: GradientBoostingClassifier .\n",
    "\n",
    "## Gradient Boosting: Cons\n",
    "- GB involves an exhaustive search procedure.\n",
    "- Each CART is trained to find the best split points and features.\n",
    "- May lead to CARTs using the same split points and maybe the same features.\n",
    "\n",
    "## Stochastic Gradient Boosting\n",
    "- Each tree is trained on a random subset of rows of the training data.\n",
    "- The sampled instances (40%-80% ofthe training set) are sampled without replacement.\n",
    "- Features are sampled (without replacement) when choosing split points.\n",
    "- Result: further ensemble diversity.\n",
    "- Effect: adding further variance to the ensemble oftrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 5 - MODEL TUNING\n",
    "\n",
    "- The hyperparameters of a machine learning model are parameters that are not learned from data.\n",
    "- They should be set prior to fitting the model to the training set.\n",
    "\n",
    "## Parameters\n",
    "- learned from data\n",
    "- CART example: split-point of a node, split-feature of a node, ...\n",
    "\n",
    "## Hyperparameters\n",
    "- not learned from data, set prior to training\n",
    "- CART example: max_depth , min_samples_leaf , splitting criterion ...\n",
    "\n",
    "## What is hyperparameter tuning?\n",
    "- Problem: search for a set of optimal hyperparameters for a learning algorithm.\n",
    "- Solution: find a set of optimal hyperparameters that results in an optimal model.\n",
    "- Optimal model: yields an optimal score.\n",
    "- Score: in sklearn defaults to accuracy (classication) and R-squared (regression).\n",
    "- Cross validation is used to estimate the generalization performance.\n",
    "\n",
    "## Why tune hyperparameters?\n",
    "- In sklearn, a model's default hyperparameters are not optimal for all problems.\n",
    "- Hyperparameters should be tuned to obtain the best model performance.\n",
    "\n",
    "## Approaches to hyperparameter tuning\n",
    "- Grid Search\n",
    "- Random Search\n",
    "- Bayesian Optimization\n",
    "- GeneticAlgorithms etc.\n",
    "\n",
    "## Grid search cross validation\n",
    "- Manually set a grid of discrete hyperparameter values.\n",
    "- Set a metric for scoring model performance.\n",
    "- Search exhaustively through the grid.\n",
    "- For each set of hyperparameters, evaluate each model's CV score.\n",
    "- The optimal hyperparameters are those ofthe model achieving the best CV score.\n",
    "\n",
    "## Grid search cross validation: example\n",
    "- Hyperparameters grids:\n",
    "- max_depth = {2,3,4},\n",
    "- min_samples_leaf = {0.05, 0.1}\n",
    "- hyperparameter space = { (2,0.05) , (2,0.1) , (3,0.05), ... }\n",
    "- CV scores = { score , ... }\n",
    "- optimal hyperparameters = set of hyperparameters corresponding to the best CV score.\n",
    "\n",
    "Tuning a RF's Hyperparameters\n",
    "- Tuning is expensive\n",
    "\n",
    "## Hyperparameter tuning:\n",
    "- computationally expensive,\n",
    "- sometimes leads to very slight improvement,\n",
    "- Weight the impact oftuning on the whole project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - CLASSIFICATION AND REGRESSION TREES (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split dataset into 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2,stratify=y,random_state=1)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate dt, set 'criterion' to 'gini'\n",
    "dt = DecisionTreeClassifier(criterion= 'gini', random_state=1)\n",
    "\n",
    "# Instantiate dt, set 'entropy' as the information criterion\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=1)\n",
    "\n",
    "# Most of the time, the gini index and entropy lead to the same results.\n",
    "# The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate test-set accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))\n",
    "\n",
    "# REGRESSION\n",
    "# Import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Split data into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor 'dt'\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=3)\n",
    "# 0.1 implies Atleast 10% of the training data\n",
    "\n",
    "# Fit 'dt' to the training-set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test-set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test-set MSE\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test-set RMSE\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(rmse_dt)\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Fold CV in regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate decision tree regressor and assign it to 'dt'\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=SEED)\n",
    "\n",
    "# Evaluate the list of MSE ontained by 10-fold CV\n",
    "# Set n_jobs to -1 in order to exploit all CPU cores in computation\n",
    "MSE_CV = - cross_val_score(dt, X_train, y_train, cv= 10, scoring= 'neg_mean_squared_error' , n_jobs = -1)\n",
    "\n",
    "# Fit 'dt' to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of training set\n",
    "y_predict_train = dt.predict(X_train)\n",
    "y_predict_train\n",
    "\n",
    "# Predict the labels of test set\n",
    "y_predict_test = dt.predict(X_test)\n",
    "\n",
    "# CV MSE\n",
    "print('CV MSE: {:.2f}'.format(MSE_CV.mean()))\n",
    "\n",
    "# Training set MSE\n",
    "print('Train MSE: {:.2f}'.format(MSE(y_train, y_predict_train)))\n",
    "\n",
    "# Test set MSE\n",
    "print('Test MSE: {:.2f}'.format(MSE(y_test, y_predict_test)))\n",
    "\n",
    "# Suppose CV MSE = 20.51, Train MSE = 15.30 and Test MSE = 20.92\n",
    "# Train MSE < CV MSE.\n",
    "# Suggested that model is overfit and is suffering from high variance.\n",
    "# CV MSE and Test MSE are roughly equal\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV.mean())**(1/2)\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
    "\n",
    "# Suppose, RMSE_CV = 5.14, RMSE_train = 5.15 and baseline_RMSE = 5.1\n",
    "# RMSE_CV < RMSE_train means dt suffers from high bias because RMSE_CV ≈ RMSE_train and both scores are greater than baseline_RMSE.\n",
    "# dt is indeed underfitting the training set as the model is too constrained to capture the nonlinear dependencies between features and labels\n",
    "\n",
    "\n",
    "# Ensemble Learning\n",
    "# Import functions to compute accuracy and split data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import models, including VotingClassifier meta-model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= SEED)\n",
    "\n",
    "# Instantiate individual classifiers\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "knn = KNN(n_neighbors=27)\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define a list called classifier that contains the tuples (classifier_name, classifier)\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n",
    "\n",
    "# Iterate over the defined list of tuples containing the classifiers\n",
    "for clf_name, clf in classifiers:\n",
    "  \n",
    "  #fit clf to the training set\n",
    "  clf.fit(X_train, y_train)\n",
    "  \n",
    "  # Predict the labels of the test set\n",
    "  y_pred = clf.predict(X_test)\n",
    "  \n",
    "  # Evaluate the accuracy of clf on the test set\n",
    "  print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "  \n",
    "# OR \n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)   \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n",
    "\n",
    "# Instantiate a VotingClassifier 'vc'\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "# Fit 'vc' to the traing set and predict test set labels\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Evaluate the test-set accuracy of 'vc'\n",
    "print('Voting Classifier: {.3f}'.format(accuracy_score(y_test, y_pred)))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Bagging and Random Forest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAGGING CLASSIFICATION\n",
    "# Import models and utility functions\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "\n",
    "# Fit 'bc' to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate and print test-set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))\n",
    "\n",
    "\n",
    "# OOB EVALUATION IN SKLEARN\n",
    "# Import models and split utility function\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, stratify= y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'; set oob_score= True\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "# Fit 'bc' to the traing set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Extract the OOB accuracy from 'bc'\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "# Print test set accuracy\n",
    "print('Test set accuracy: {:.3f}'.format(test_accuracy))\n",
    "\n",
    "# Print OOB accuracy\n",
    "print('OOB accuracy: {:.3f}'.format(oob_accuracy))\n",
    "# The difference between test and oob accuracy will be minimal which proved that we don't need cross validation to check the model accuracy\n",
    "\n",
    "\n",
    "# RANDOM FOREST REGRESSOR\n",
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a random forests regressor 'rf' 400 estimators\n",
    "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "# Fit 'rf' to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "\n",
    "\n",
    "# FEATURE IMPORTANCE in sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X_train.columns)\n",
    "#importances_rf = pd.DataFrame(rf.feature_importances_,\n",
    "#                                   index = X_train.columns,\n",
    "#                                    columns=['importance']).sort_values('importance',  \n",
    "#ascending=False)\n",
    "\n",
    "# Sort importances_rf\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "# Make a horizontal bar plot\n",
    "sorted_importances_rf.plot(kind= 'barh', color= 'lightgreen'); \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# AdaBoost Classication in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "# Instantiate an AdaBoost classifier 'adab_clf'\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "# Fit 'adb_clf' to the training set\n",
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set probabilities of positive class\n",
    "# Once the classifier adb_clf is trained, call the .predict_proba() method by passing X_test as a parameter \n",
    "# Extract these probabilities by slicing all the values in the second column as follows\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "\n",
    "\n",
    "adb_clf_roc_auc_score= roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print adb_clf_roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))\n",
    "\n",
    "\n",
    "# Gradient Boosting in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate a GradientBoostingRegressor 'gbt'\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
    "\n",
    "# Fit 'gbt' to the training set\n",
    "gbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))\n",
    "\n",
    "\n",
    "# Stochastic Gradient Boosting in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate a stochastic GradientBoostingRegressor 'sgbt'\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=SEED)\n",
    "# 0.8 refers to sample 80% of datafor training\n",
    "# 0.2 refers to each tree uses 20% of the available features to perform best split\n",
    "\n",
    "# Fit 'sgbt' to the training set\n",
    "sgbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "# Evaluate test set RMSE 'rmse_test'\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print 'rmse_test'\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Model Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting the hyperparameters of a CART\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set seed to 1 for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt'\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# Print out 'dt's hyperparameters\n",
    "print(dt.get_params())\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of hyperparameters 'params_dt'\n",
    "params_dt = {\n",
    "              'max_depth': [3, 4,5, 6],\n",
    "              'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "              'max_features': [0.2, 0.4,0.6, 0.8]\n",
    "            }\n",
    "\n",
    "# Instantiate a 10-fold CV grid search object 'grid_dt'\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "\n",
    "# Fit 'grid_dt' to the training data\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Extract best hyperparameters from 'grid_dt'\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print('Best hyerparameters:\\n', best_hyperparams)\n",
    "\n",
    "# Extract best CV score from 'grid_dt'\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print('Best CV accuracy'.format(best_CV_score))\n",
    "\n",
    "# Extract best model from 'grid_dt'\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "test_acc = best_model.score(X_test,y_test)\n",
    "\n",
    "# Print test set accuracy\n",
    "print(\"Test set accuracy of best model: {:.3f}\".format(test_acc))\n",
    "\n",
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))\n",
    "\n",
    "\n",
    "# Inspecting RF Hyperparameters in sklearn\n",
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate a random forests regressor 'rf'\n",
    "rf = RandomForestRegressor(random_state= SEED)\n",
    "\n",
    "# Inspect rf' s hyperparameters\n",
    "rf.get_params()\n",
    "\n",
    "# Basic imports\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameter 'params_rf'\n",
    "params_rf = {\n",
    "                'n_estimators': [300, 400, 500],\n",
    "                'max_depth': [4, 6, 8],\n",
    "                'min_samples_leaf': [0.1, 0.2],\n",
    "                'max_features': ['log2','sqrt']\n",
    "            }\n",
    "\n",
    "# Instantiate 'grid_rf'\n",
    "grid_rf = GridSearchCV(estimator=rf,param_grid=params_rf, cv=3, scoring= 'neg_mean_squared_error',verbose=1, n_jobs=-1)\n",
    "\n",
    "# Searching for the best hyperparameters\n",
    "# Fit 'grid_rf' to the training set\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract best hyperparameters from 'grid_rf'\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "print('Best hyerparameters:\\n', best_hyperparams)\n",
    "\n",
    "# Extract best model from 'grid_rf'\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295.457px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
