{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536983d8",
   "metadata": {},
   "source": [
    "# Project 1 - Linear Regression and Gradient Descent\n",
    "\n",
    "To develop python code modules, you need to use python IDEs. There are many options. Anacoda supports `spyder`and it is included in the installation. I like the simple python IDE `rodeo` that can be downloaded from `https://sourceforge.net/projects/rodeo.mirror/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4eb92b",
   "metadata": {},
   "source": [
    "# Computing the Cost Function\n",
    "In this exercise, we will focus on simple linear regression which takes the following form,\n",
    "$$\n",
    "y_n \\approx f\\left(x_{n 1}\\right)=w_0+w_1 x_{n 1} .\n",
    "$$\n",
    "We will use height as the input variable $x_{n 1}$ and weight as the output variable $y_n$. The coefficients $w_0$ and $w_1$ are also called model parameters. We will use a mean-square-error (MSE) function defined as follows,\n",
    "$$\n",
    "\\mathcal{L}\\left(w_0, w_1\\right)=\\frac{1}{2 N} \\sum_{n=1}^N\\left(y_n-f\\left(x_{n 1}\\right)\\right)^2=\\frac{1}{2 N} \\sum_{n=1}^N\\left(y_n-w_0-w_1 x_{n 1}\\right)^2 .\n",
    "$$\n",
    "Our goal is to find $w_0^{\\star}$ and $w_1^{\\star}$ that minimize this cost.\n",
    "Let us start by the array data type in NumPy. We store all the $\\left(y_n, x_{n}\\right)$ pairs in a vector and a matrix as shown below.\n",
    "$$\n",
    "\\boldsymbol{y}=\\left[\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{array}\\right] \\quad \\widetilde{\\boldsymbol{X}}=\\left[\\begin{array}{cc}\n",
    "1 & x_{11} \\\\\n",
    "1 & x_{21} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_{N 1}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492bf6e",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "For this exercise and the rest,  you will use the dataset `height_weight_genders.csv'.\n",
    "\n",
    "To understand this data format, answer the following warm up questions:\n",
    "- What does each column of $\\tilde{\\boldsymbol{X}}$ represent?\n",
    "- What does each row of $\\tilde{\\boldsymbol{X}}$ represent?\n",
    "- Why do we have 1 's in $\\tilde{\\boldsymbol{X}}$ ?\n",
    "- If we have heights and weights of 3 people, what would be the size of $\\boldsymbol{y}$ and $\\tilde{\\boldsymbol{X}}$ ? What would $\\tilde{\\boldsymbol{X}}_{32}$ represent?\n",
    "- In helpers.py, we have already provided code to form arrays for $\\boldsymbol{y}$ and $\\tilde{\\boldsymbol{X}}$. Have a look at the code, and make sure you understand how they are constructed.\n",
    "- Check if the sizes of the variables make sense (use functions shape).\n",
    "a) Now we will compute the MSE. Let us introduce the vector notation $\\boldsymbol{e}=\\boldsymbol{y}-\\tilde{\\boldsymbol{X}} \\boldsymbol{w}$, for given model parameters $\\boldsymbol{w}=\\left[w_0, w_1\\right]^{\\top}$. Prove that the MSE can also be rewritten in terms of the vector $\\boldsymbol{e}$, as\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{w})=\\ldots\n",
    "$$\n",
    "b) Complete the implementation of the notebook function compute_loss(y, tx, w). You can start by setting $\\boldsymbol{w}=[1,2]^{\\top}$, and test your function.\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37458b91",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Now we are ready to implement our first optimization algorithm: Grid Search. \n",
    "\n",
    "\\begin{exercise}\n",
    "\n",
    "\n",
    "\n",
    "a) Fill in the notebook function `grid_search (y, tx, w0, w1)` to implement grid search. You will have to write one for-loop per dimension, and compute the cost function for each setting of $w_0$ and $w_1$. Once you have all values of cost function stored in the variable loss, the code finds an approximate minimum (as discussed in the class).\n",
    "\n",
    "The code should print the obtained minimum value of the cost function along with the found $w_0^{\\star}$ and $w_1^{\\star}$. It should also show a contour plot and the plot of the fit, as shown in Figure 1.\n",
    "![](https://cdn.mathpix.com/snip/images/E__D-Bm6Q_pufTuJl3gkojp7ctW6FNKHdgz9uyur8d8.original.fullsize.png)\n",
    "\n",
    "b) Does this look like a good estimate? Why not? What is the problem? Why is the MSE plot not smooth? Repeat the above exercise by changing the grid spacing to 10 instead of 50 . Compare the new fit to the old one.\n",
    "c) Discuss with your peers:\n",
    "- To obtain an accurate fit, do you need a coarse grid or a fine grid?\n",
    "- Try different values of grid spacing. What do you observe?\n",
    "- How does increasing the number of values affect the computational cost? How fast or slow does your code run?\n",
    "\n",
    "\\end{exercise}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e80889c",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent\n",
    "\\begin{exercise}\n",
    "\n",
    "Derive the following expressions for the gradient (the vector of partial derivatives) of the MSE for linear regression,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial \\mathcal{L}\\left(w_0, w_1\\right)}{\\partial w_0}=-\\frac{1}{N} \\sum_{n=1}^N\\left(y_n-w_0-w_1 x_{n 1}\\right)=-\\frac{1}{N} \\sum_{n=1}^N e_n \\\\\n",
    "& \\frac{\\partial \\mathcal{L}\\left(w_0, w_1\\right)}{\\partial w_1}=-\\frac{1}{N} \\sum_{i=1}^N\\left(y_n-w_0-w_1 x_{n 1}\\right) x_{n 1}=-\\frac{1}{N} \\sum_{n=1}^N e_n x_{n 1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Denoting the gradient by $\\nabla \\mathcal{L}(\\boldsymbol{w})$, we can write these operations in vector form as follows,\n",
    "$$\n",
    "\\nabla \\mathcal{L}(\\boldsymbol{w}):=\\left[\\frac{\\partial \\mathcal{L}\\left(w_0, w_1\\right)}{\\partial w_0} \\frac{\\partial \\mathcal{L}\\left(w_0, w_1\\right)}{\\partial w_1}\\right]=-\\frac{1}{N}\\left[\\begin{array}{c}\n",
    "\\sum_{n=1}^N e_n \\\\\n",
    "\\sum_{n=1}^N e_n x_{n 1}\n",
    "\\end{array}\\right]=-\\frac{1}{N} \\tilde{\\boldsymbol{X}}^{\\top} \\boldsymbol{e}\n",
    "$$\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f9d328",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "\n",
    "a) Now implement a function that computes the gradients. Implement the notebook function `compute_gradient (y, tx, w)` using Equation (6). Verify that the function returns the right values. First, manually compute the gradients for hand-picked values of $\\boldsymbol{y}, \\tilde{\\boldsymbol{X}}$, and $\\boldsymbol{w}$ and compare them to the output of compute_gradient.\n",
    "\n",
    "b) Once you make sure that your gradient code is correct, get some intuition about the gradient values:\n",
    "Compute the gradients for\n",
    "- $w_0=100$ and $w_1=20$\n",
    "- $w_0=50$ and $w_1=10$\n",
    "What do the values of these gradients tell us? For example, think about the norm of this vector. In which case are they bigger? What does that mean?\n",
    "Hint: Imagine a quadratic function and estimate its gradient near its minimum and far from it.\n",
    "Hint 2: As we know from the lecture, the update rule for gradient descent at step $t$ is\n",
    "$$\n",
    "\\boldsymbol{w}^{(t+1)}=\\boldsymbol{w}^{(t)}-\\gamma \\nabla \\mathcal{L}\\left(\\boldsymbol{w}^{(t)}\\right)\n",
    "$$\n",
    "where $\\gamma>0$ is the step size, and $\\nabla \\mathcal{L} \\in \\mathbb{R}^2$ is the gradient vector.\n",
    "\n",
    "c) Fill in the notebook function `gradient_descent (y, tx, initial_w, ...)`. Run the code and visualize the iterations. Also, look at the printed messages that show $\\mathcal{L}$ and values of $w_0^{(t)}$ and $w_1^{(t)}$. Take a detailed look at these plots,\n",
    "- Is the cost being minimized?\n",
    "- Is the algorithm converging? What can be said about the convergence speed?\n",
    "- How good are the final values of $w_1$ and $w_0$ found?\n",
    "\n",
    "d) Now let's experiment with the value of the step size and initialization parameters and see how they influences the convergence. In theory, gradient descent converges to the optimum on convex functions, when the value of the step size is chosen appropriately.\n",
    "- Try the following values of step size: $0.001,0.01,0.5,1,2,2.5$. What do you observe? Did the procedure converge?\n",
    "- Try different initializations with fixed step size $\\gamma=0.1$, for instance:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& -w_0=0, w_1=0 \\\\\n",
    "& -w_0=100, w_1=10 \\\\\n",
    "& -w_0=-1000, w_1=1000\n",
    "\\end{aligned}\n",
    "$$\n",
    "What do you observe? Did the procedure converge?\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2939174",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\\begin{exercise}\n",
    "\n",
    "Let us implement stochastic gradient descent. Recall from the lecture notes that the update rule for stochastic gradient descent on an objective function $\\mathcal{L}(\\boldsymbol{w})=\\frac{1}{N} \\sum_{n=1}^N \\mathcal{L}_n(\\boldsymbol{w})$ at step $t$ is\n",
    "$$\n",
    "\\boldsymbol{w}^{(t+1)}=\\boldsymbol{w}^{(t)}-\\gamma \\nabla \\mathcal{L}_n\\left(\\boldsymbol{w}^{(t)}\\right)\n",
    "$$\n",
    "HINT: You can use the function `batch_iter()` in the file of `helpers.py` to generate mini-batch data for stochastic gradient descent.\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012bfe02",
   "metadata": {},
   "source": [
    "# Subgradient Descent\n",
    "\n",
    "\\begin{exercise}\n",
    "\n",
    "Modify the function `compute_loss ( $y, t x, w)$` for the Mean Absolute Error cost function.\n",
    "Unfortunately, you cannot directly use gradient descent, since the MAE function is non-differentiable at several points.\n",
    "\n",
    "a) Compute a subgradient of the MAE cost function, for every given vector $\\boldsymbol{w}$.\n",
    "Hint: Use the chain rule to compute the subgradient of the absolute value function. For a function $\\mathcal{L}(\\boldsymbol{w}):=h(q(\\boldsymbol{w}))$ with $q$ differentiable, the subgradient can be computed using $\\partial \\mathcal{L}(\\boldsymbol{w})=\\partial h(q(\\boldsymbol{w})) \\cdot \\nabla q(\\boldsymbol{w})$, where each $\\partial$.. denotes the set of all subgradient vectors.\n",
    "\n",
    "b) Implement subgradient descent for the MAE cost function.\n",
    "To do so, write a new function `compute_subgradient( $y, t x$, w)` for the new MAE objective which returns a subgradient if the given $\\boldsymbol{w}$ turns out to be a non-differentiable point.\n",
    "Plot the resulting model $f$ together with the two curves obtained in the previous exercise.\n",
    "- Is the fit using MAE better than the one using MSE?\n",
    "- Did your optimization algorithm ever encounter a non-differentiable point?\n",
    "\n",
    "c) Implement stochastic subgradient descent (SGD) for the MAE cost function.\n",
    "How is the picture different when you compare the two algorithm variants on MAE, compared to what you have observed on MSE?\n",
    "\\end{exercise}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9471c",
   "metadata": {},
   "source": [
    "# Important Note\n",
    "\n",
    "After you have finished the implementation of the above exercises you will incorporate your code in the notebook Lab02.ipynb, you can wrap up by copying your code to separate .py files for later re-use. For example, you'll be re-using your code from this week later on, Projects and some of the subsequent labs.\n",
    "You will find template files for this, namely\n",
    "`cost.py`, `rid_search.py`, `gradient_descent.py` and `stochastic_gradient_descent.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
