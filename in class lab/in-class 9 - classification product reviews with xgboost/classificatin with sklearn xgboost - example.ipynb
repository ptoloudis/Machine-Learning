{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70af64c",
   "metadata": {},
   "source": [
    "# sklearn XGBoost classifier\n",
    "\n",
    "The sklearn XGBoost classifier is an implementation of the popular gradient boosting algorithm called XGBoost (eXtreme Gradient Boosting). XGBoost is known for its high performance and efficiency in handling structured data and achieving excellent results in machine learning tasks, particularly in classification problems.\n",
    "\n",
    "The XGBoost classifier in sklearn combines multiple weak prediction models (typically decision trees) to create a strong predictive model. It does this through an iterative process, where each subsequent model is trained to correct the mistakes of the previous models. The final prediction is obtained by summing the predictions of all individual models, weighted by their importance.\n",
    "\n",
    "Here are some key features and characteristics of the sklearn XGBoost classifier:\n",
    "\n",
    "1. Boosting: XGBoost is a boosting algorithm, which means it builds a strong model by combining several weak models sequentially. Each weak model is trained to correct the errors made by the previous models.\n",
    "\n",
    "2. Gradient Boosting: XGBoost uses gradient boosting, which involves minimizing a loss function by iteratively fitting new models to the negative gradient of the loss function. This approach enables the model to learn complex patterns and make accurate predictions.\n",
    "\n",
    "3. Regularization: XGBoost provides regularization techniques to prevent overfitting. It includes L1 and L2 regularization terms in the objective function, which helps to control the complexity of the model and improve its generalization ability.\n",
    "\n",
    "4. Parallel Processing: XGBoost supports parallel processing, allowing for faster training and prediction on multi-core CPUs. It utilizes multiple threads to efficiently handle large datasets.\n",
    "\n",
    "5. Feature Importance: XGBoost provides a feature importance mechanism that ranks the importance of each feature in the dataset. This information can be useful for feature selection and understanding the factors driving the predictions.\n",
    "\n",
    "6. Hyperparameter Tuning: XGBoost offers a wide range of hyperparameters that can be tuned to optimize the model's performance. These include parameters related to tree structure, learning rate, regularization, and more.\n",
    "\n",
    "7. Integration with sklearn: The XGBoost classifier in sklearn follows the scikit-learn API conventions, making it easy to integrate into existing sklearn workflows and take advantage of sklearn's utilities for preprocessing, cross-validation, and evaluation.\n",
    "\n",
    "Overall, the sklearn XGBoost classifier is a powerful and versatile tool for classification tasks, known for its accuracy, scalability, and ability to handle complex data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd615e3",
   "metadata": {},
   "source": [
    "# classification example with xgboost\n",
    "\n",
    "In this example, we first import the necessary libraries, including numpy, pandas, and xgboost. We then load the Breast Cancer dataset from sklearn using load_breast_cancer.\n",
    "\n",
    "\n",
    "Next, we split the dataset into training and testing sets using train_test_split from sklearn. We convert the training and testing data into DMatrix format, which is the internal data structure used by XGBoost.\n",
    "\n",
    "\n",
    "We set the XGBoost parameters such as max_depth, eta (learning rate), objective (binary logistic regression in this case), and eval_metric (log loss for evaluation).\n",
    "\n",
    "\n",
    "After that, we train the XGBoost model using xgb.train by passing the parameters, training data, and the number of training rounds.\n",
    "\n",
    "\n",
    "Finally, we make predictions on the test set, convert the predicted probabilities to binary predictions, and calculate the accuracy score using accuracy_score from sklearn. The accuracy score is then printed.\n",
    "\n",
    "\n",
    "You can run this code in a Jupyter Notebook and modify it according to your own needs or dataset. Just make sure you have installed the necessary dependencies (xgboost, numpy, pandas, scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a404decb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset from sklearn\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into DMatrix format for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set the XGBoost parameters\n",
    "params = {\n",
    "    'max_depth': 3,          # Maximum depth of a tree\n",
    "    'eta': 0.1,              # Learning rate\n",
    "    'objective': 'binary:logistic',  # Objective function\n",
    "    'eval_metric': 'logloss'  # Evaluation metric\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred_binary = np.round(y_pred)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate and print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
